{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f73173a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的包\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cd73e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flight_data = pd.read_csv('flighttimechange_final.csv')\n",
    "all_airports = pd.read_csv('DirectoryList2.csv')\n",
    "\n",
    "\n",
    "columns_from_weather = [\n",
    "    'DATE','NAME','REPORT_TYPE','HourlyPrecipitation', 'HourlyRelativeHumidity', 'HourlySkyConditions',\n",
    "    'HourlyStationPressure', 'HourlyVisibility', 'HourlyWetBulbTemperature',\n",
    "    'HourlyWindGustSpeed', 'HourlyWindSpeed','DailySnowDepth','DailySnowfall']\n",
    "\n",
    "columns_drop_from_flight = ['QUARTER' , 'FL_DATE' , 'SCH_OP_CARRIER' , 'SCH_OP_CARRIER_FL_NUM' , 'OP_CARRIER' ,\n",
    "                            'ORIGIN_AIRPORT_ID' , 'ORIGIN_AIRPORT_SEQ_ID' , 'ORIGIN_CITY_MARKET_ID' ,\n",
    "                       'ORIGIN_STATE_ABR' , 'ORIGIN_STATE_NM', 'DEST_AIRPORT_ID' , 'DEST_AIRPORT_SEQ_ID' , 'DEST_CITY_MARKET_ID' ,\n",
    "                       'DEST_STATE_ABR' , 'DEST_STATE_NM' , \n",
    "                       'DEP_TIME_BLK' , 'WHEELS_OFF' , 'WHEELS_ON' , 'ARR_TIME_BLK' ,\n",
    "                       'AIR_TIME' , 'FLIGHTS' , 'CRS_ELAPSED_TIME','CANCELLATION_CODE']\n",
    "\n",
    "\n",
    "flight_data = flight_data.drop(columns_drop_from_flight, axis=1)\n",
    "\n",
    "# diverted is not considered\n",
    "flight_data = flight_data[flight_data['DIVERTED']==0]\n",
    "# not cancelled but no arrival time - one row\n",
    "flight_data = flight_data.drop(flight_data[((flight_data['CANCELLED'] == 0) & (flight_data['ARR_TIME'].isna()))].index)\n",
    "# not cancelled, delayed, but no delay reason\n",
    "flight_data = flight_data.drop(flight_data[(flight_data['CANCELLED']==0) & (flight_data['ARR_DELAY'] > 30) & (flight_data['CARRIER_DELAY'].isna())].index)\n",
    "\n",
    "\n",
    "flight_data['FL_DATE'] = pd.to_datetime(flight_data['Timechange_FL_DATE'])\n",
    "# flight_data['FL_TIME'] = pd.to_datetime(flight_data['CRS_DEP_TIME'], format='%H%M', errors='coerce').dt.time\n",
    "\n",
    "flight_data['FL_TIME'] = flight_data['Timechange_CRS_DEP_Time']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine departure airport weather at scheduled departure time\n",
    "\n",
    "\n",
    "combined_pt1 = pd.DataFrame()\n",
    "manual_handle = []\n",
    "\n",
    "\n",
    "\n",
    "for i,airport in all_airports.iterrows():\n",
    "    print(i)\n",
    "    if pd.isna(airport['ReplaceID']):\n",
    "        StationID = airport['StationID']\n",
    "    else:\n",
    "        StationID = airport['ReplaceID']\n",
    "\n",
    "    airportCode = airport['iata_code']\n",
    "    weather_data = pd.read_csv(os.path.join('Weather_DATA_IN_CST_filled2',StationID+'.csv'), usecols=columns_from_weather)\n",
    "    weather_data = weather_data.loc[(weather_data['REPORT_TYPE']!='SOD') & (weather_data['REPORT_TYPE']!='SOM')]\n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "    weather_data['timestamp'] = weather_data['DATE']\n",
    "\n",
    "    # Weather_origin\n",
    "    weather_data_orig = weather_data.copy()\n",
    "    weather_data_orig.columns = [col+'_ORIGIN' for col in list(weather_data_orig.columns)]\n",
    "    weather_data_orig['timestamp'] = weather_data_orig['timestamp_ORIGIN']\n",
    "\n",
    "\n",
    "\n",
    "    flight_by_origin = flight_data[flight_data['ORIGIN'] == airportCode]\n",
    "#     flight_by_destination = flight_data[flight_data['DEST'] == airportCode]\n",
    "    flight_by_origin['timestamp'] = pd.to_datetime(flight_by_origin['FL_DATE'].astype(str) + ' ' + flight_by_origin['FL_TIME'].astype(str), errors='coerce')\n",
    "#     flight_by_destination['timestamp'] = pd.to_datetime(flight_by_destination['FL_DATE'].astype(str) + ' ' + flight_by_destination['FL_TIME'].astype(str), errors='coerce')\n",
    "\n",
    "\n",
    "    # 处理flight NA值\n",
    "    if len(flight_by_origin[flight_by_origin['timestamp'].isna()]) > 0:\n",
    "        manual_handle.append([manual_handle, flight_by_origin[flight_by_origin['timestamp'].isna()]])\n",
    "        flight_by_origin.drop(flight_by_origin[flight_by_origin['timestamp'].isna()].index, inplace=True)\n",
    "\n",
    "   \n",
    "        \n",
    "    flight_by_origin['index'] = flight_by_origin.index.copy()\n",
    "    \n",
    "    \n",
    "    combined_origin = pd.merge_asof(flight_by_origin.sort_values('timestamp'), \n",
    "                                  weather_data_orig.sort_values('timestamp'), \n",
    "                                  on='timestamp', \n",
    "                                  direction='nearest')\n",
    "    \n",
    "    combined_origin = combined_origin.reset_index().set_index('index').drop('level_0',axis=1)\n",
    "\n",
    "    combined_origin = combined_origin[[col+'_ORIGIN' for col in columns_from_weather]]\n",
    "    combined_pt1 = pd.concat([combined_pt1, flight_data.merge(combined_origin,left_index=True, right_index=True, how='inner')])\n",
    "\n",
    "\n",
    "combined_pt1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pt1.to_csv('flight_weather_V9_Pt1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07408b27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine arrival airport weather at scheduled departure time\n",
    "\n",
    "combined_pt2 = pd.DataFrame()\n",
    "manual_handle = []\n",
    "\n",
    "\n",
    "\n",
    "for i,airport in all_airports.iterrows():\n",
    "    print(i)\n",
    "    if pd.isna(airport['ReplaceID']):\n",
    "        StationID = airport['StationID']\n",
    "    else:\n",
    "        StationID = airport['ReplaceID']\n",
    "    airportCode = airport['iata_code']\n",
    "    \n",
    "    weather_data = pd.read_csv(os.path.join('Weather_DATA_IN_CST_filled2',StationID+'.csv'), usecols=columns_from_weather)\n",
    "    weather_data = weather_data.loc[(weather_data['REPORT_TYPE']!='SOD') & (weather_data['REPORT_TYPE']!='SOM')]\n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "    weather_data['timestamp'] = weather_data['DATE']\n",
    "\n",
    "    # Weather_dest\n",
    "    weather_data_dest = weather_data.copy()\n",
    "    weather_data_dest.columns = [col+'_DEST' for col in list(weather_data_dest.columns)]\n",
    "    weather_data_dest['timestamp'] = weather_data_dest['timestamp_DEST']\n",
    "\n",
    "\n",
    "\n",
    "    flight_by_dest = combined_pt1[combined_pt1['DEST'] == airportCode]\n",
    "#     flight_by_destination = flight_data[flight_data['DEST'] == airportCode]\n",
    "    flight_by_dest['timestamp'] = pd.to_datetime(flight_by_dest['FL_DATE'].astype(str) + ' ' + flight_by_dest['FL_TIME'].astype(str), errors='coerce')\n",
    "#     flight_by_destination['timestamp'] = pd.to_datetime(flight_by_destination['FL_DATE'].astype(str) + ' ' + flight_by_destination['FL_TIME'].astype(str), errors='coerce')\n",
    "\n",
    "\n",
    "    # 处理flight NA值\n",
    "    if len(flight_by_dest[flight_by_dest['timestamp'].isna()]) > 0:\n",
    "        manual_handle.append([manual_handle, flight_by_dest[flight_by_dest['timestamp'].isna()]])\n",
    "        flight_by_dest.drop(flight_by_dest[flight_by_dest['timestamp'].isna()].index, inplace=True)\n",
    "\n",
    "   \n",
    "        \n",
    "    flight_by_dest['index'] = flight_by_dest.index.copy()\n",
    "    \n",
    "    \n",
    "    combined_dest = pd.merge_asof(flight_by_dest.sort_values('timestamp'), \n",
    "                                  weather_data_dest.sort_values('timestamp'), \n",
    "                                  on='timestamp', \n",
    "                                  direction='nearest')\n",
    "    \n",
    "    combined_dest = combined_dest.reset_index().set_index('index').drop('level_0',axis=1)\n",
    "    combined_dest = combined_dest[[col+'_DEST' for col in columns_from_weather]]\n",
    "    combined_pt2 = pd.concat([combined_pt2, combined_pt1.merge(combined_dest,left_index=True, right_index=True, how='inner')])\n",
    "\n",
    "    \n",
    "#                                pd.concat([combined_origin, combined_dest])])\n",
    "combined_pt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_V9 = combined_pt2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_V9.to_csv('flight_weather_V9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER OUT MISMATCHED DATES (>12HR)\n",
    "\n",
    "\n",
    "file_path = \"flight_weather_V9_1.csv\"\n",
    "output_path = \"V9_1_filtered.csv\"\n",
    "\n",
    "# 定义chunksize以分块读取数据\n",
    "chunksize = 10000  # 可根据内存情况调整块大小\n",
    "first_chunk = True  # 标记是否是第一个块，以便写入表头\n",
    "\n",
    "# 逐块读取并处理数据\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    # 将 DATE_ORIGIN 和 DATE_DEST 列转换为 datetime 格式\n",
    "    chunk['DATE_ORIGIN'] = pd.to_datetime(chunk['DATE_ORIGIN'], errors='coerce')\n",
    "    chunk['DATE_DEST'] = pd.to_datetime(chunk['DATE_DEST'], errors='coerce')\n",
    "    \n",
    "    # 合并 Timechange_FL_DATE 和 Timechange_CRS_DEP 列为一个 datetime 列\n",
    "    chunk['COMBINED_DATETIME'] = pd.to_datetime(\n",
    "        chunk['Timechange_FL_DATE'].astype(str) + ' ' + chunk['Timechange_CRS_DEP_Time'].astype(str),\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # 计算与 DATE_ORIGIN 的时间差（以小时为单位）\n",
    "    chunk['time_diff_origin'] = (chunk['COMBINED_DATETIME'] - chunk['DATE_ORIGIN']).abs() / pd.Timedelta(hours=1)\n",
    "    \n",
    "    # 计算与 DATE_DEST 的时间差（以小时为单位）\n",
    "    chunk['time_diff_dest'] = (chunk['COMBINED_DATETIME'] - chunk['DATE_DEST']).abs() / pd.Timedelta(hours=1)\n",
    "\n",
    "    # 筛选时间差超过 12 小时的记录，并标记是 DATE_ORIGIN 还是 DATE_DEST 问题\n",
    "    filtered_chunk = chunk[(chunk['time_diff_origin'] > 12) | (chunk['time_diff_dest'] > 12)].copy()\n",
    "#     filtered_chunk['Exceeds_12_Hours'] = filtered_chunk.apply(\n",
    "#         lambda x: 'DATE_ORIGIN' if x['time_diff_origin'] > 12 else 'DATE_DEST',\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "    # 将筛选出的数据追加写入CSV文件，只有第一个块写入表头\n",
    "    filtered_chunk.to_csv(output_path, mode='w' if first_chunk else 'a', index=False, header=first_chunk)\n",
    "    first_chunk = False  # 后续块不写入表头\n",
    "\n",
    "print(f\"时间差超过 12 小时的记录已逐块保存至 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a1190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shixin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (44,47,48,49,51,53,57,60,61,62) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "flight_V9_1 = pd.read_csv('flight_weather_V9_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba926064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDED Y COLUMN\n",
    "def time_to_minutes(time_str):\n",
    "    \"\"\"将时间字符串转换为当天的分钟数。如果时间为空，则返回 NaN。\"\"\"\n",
    "    if time_str == 'Invalid Time':\n",
    "        return 0\n",
    "    hours, minutes = map(int, time_str.split(':'))\n",
    "    return hours * 60 + minutes\n",
    "\n",
    "flight_V9_1['ARR_Time_min'] = flight_V9_1['Timechange_ARR_Time'].apply(time_to_minutes)\n",
    "flight_V9_1['DEP_Time_min'] = flight_V9_1['Timechange_CRS_DEP_Time'].apply(time_to_minutes)\n",
    "\n",
    "# 处理跨日情况\n",
    "flight_V9_1['Y'] = np.where(flight_V9_1['ARR_Time_min'] < flight_V9_1['DEP_Time_min'],\n",
    "                   flight_V9_1['ARR_Time_min'] + 1440 - flight_V9_1['DEP_Time_min'],  # 到达次日\n",
    "                   flight_V9_1['ARR_Time_min'] - flight_V9_1['DEP_Time_min'])          # 到达当天\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c577537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDED DELAYED COLUMN\n",
    "flight_V9_1['DELAYED'] = flight_V9_1['ARR_DELAY'].apply(lambda x: 1 if x > 15 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51eca86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE DAYS AFTER HOLIDAY COLUMNS\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# 将日期列转换为日期格式\n",
    "flight_V9_1['Timechange_FL_DATE'] = pd.to_datetime(flight_V9_1['Timechange_FL_DATE'], errors='coerce')\n",
    "\n",
    "# 提取年份，并处理1月的日期\n",
    "flight_V9_1['Year'] = flight_V9_1['Timechange_FL_DATE'].dt.year\n",
    "flight_V9_1.loc[flight_V9_1['Timechange_FL_DATE'].dt.month == 1, 'Year'] -= 1\n",
    "\n",
    "# 定义节假日日期并转换成 DataFrame 格式\n",
    "holiday_data = {\n",
    "    'Year': list(range(2017, 2024)),\n",
    "    'Thanksgiving': [datetime(year, 11, 23) if year == 2017 else datetime(year, 11, 22) if year == 2018 else \n",
    "                     datetime(year, 11, 28) if year == 2019 else datetime(year, 11, 26) if year == 2020 else \n",
    "                     datetime(year, 11, 25) if year == 2021 else datetime(year, 11, 24) if year == 2022 else \n",
    "                     datetime(year, 11, 23) for year in range(2017, 2024)],\n",
    "    'Christmas': [datetime(year, 12, 25) for year in range(2017, 2024)],\n",
    "    'New Year': [datetime(year + 1, 1, 1) for year in range(2017, 2024)]\n",
    "}\n",
    "holiday_df = pd.DataFrame(holiday_data)\n",
    "\n",
    "# 将节假日数据合并到原始数据上\n",
    "flight_V9_1 = flight_V9_1.merge(holiday_df, on='Year', how='left')\n",
    "\n",
    "# 计算每个节假日的天数差异\n",
    "flight_V9_1['Days_after_Thanksgiving'] = (flight_V9_1['Timechange_FL_DATE'] - flight_V9_1['Thanksgiving']).dt.days\n",
    "flight_V9_1['Days_after_Christmas'] = (flight_V9_1['Timechange_FL_DATE'] - flight_V9_1['Christmas']).dt.days\n",
    "flight_V9_1['Days_after_New_Year'] = (flight_V9_1['Timechange_FL_DATE'] - flight_V9_1['New Year']).dt.days\n",
    "\n",
    "# 删除不需要的列\n",
    "flight_V9_1.drop(columns=['Year', 'Thanksgiving', 'Christmas', 'New Year','FL_DATE','FL_TIME'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1120b76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No columns with mixed data types (float and str) found.\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with mixed data types (float and str)\n",
    "\n",
    "mixed_type_columns = []\n",
    "for col in flight_V9_1.columns:\n",
    "    # Check if the column has both float and str types\n",
    "    unique_types = set(type(val) for val in flight_V9_1[col].dropna())\n",
    "    if float in unique_types and str in unique_types:\n",
    "        mixed_type_columns.append(col)\n",
    "        print(f\"Column '{col}' has mixed types: {unique_types}\")\n",
    "\n",
    "# Output columns with mixed types\n",
    "if mixed_type_columns:\n",
    "    print(\"\\nColumns with mixed data types (float and str):\")\n",
    "    print(mixed_type_columns)\n",
    "else:\n",
    "    print(\"No columns with mixed data types (float and str) found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47ff27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the VISIBILITY values\n",
    "def clean_visibility(value):\n",
    "    if isinstance(value, str):\n",
    "        # If the value ends with 'V' or 's', remove the last character\n",
    "        if value.endswith('V') or value.endswith('s'):\n",
    "            value = value[:-1]\n",
    "        # If the value is '*', consider it as missing (NaN)\n",
    "        elif value == '*':\n",
    "            return np.nan\n",
    "    # Convert the cleaned value to float if possible\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan  # Return NaN if conversion to float fails\n",
    "\n",
    "# Apply the cleaning function to the column\n",
    "flight_V9_1['HourlyVisibility_ORIGIN'] = flight_V9_1['HourlyVisibility_ORIGIN'].apply(clean_visibility)\n",
    "flight_V9_1['HourlyVisibility_DEST'] = flight_V9_1['HourlyVisibility_DEST'].apply(clean_visibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c580b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the WINDSPEED values\n",
    "def clean_windspeed(value):\n",
    "    if isinstance(value, str):\n",
    "        # If the value ends with 'V' or 's', remove the last character\n",
    "        if value.endswith('s'):\n",
    "            value = value[:-1]\n",
    "    # Convert the cleaned value to float if possible\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan  # Return NaN if conversion to float fails\n",
    "\n",
    "# Apply the cleaning function to the column\n",
    "flight_V9_1['HourlyWindSpeed_ORIGIN'] = flight_V9_1['HourlyWindSpeed_ORIGIN'].apply(clean_windspeed)\n",
    "flight_V9_1['HourlyWindSpeed_DEST'] = flight_V9_1['HourlyWindSpeed_DEST'].apply(clean_windspeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53764366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the PRECIPITATION values\n",
    "def clean_precipitation(value):\n",
    "    if isinstance(value, str):\n",
    "        # If the value ends with 'V' or 's', remove the last character\n",
    "        if value=='T':\n",
    "            value = 0.01\n",
    "    # Convert the cleaned value to float if possible\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return np.nan  # Return NaN if conversion to float fails\n",
    "\n",
    "# Apply the cleaning function to the column\n",
    "flight_V9_1['HourlyPrecipitation_ORIGIN'] = flight_V9_1['HourlyPrecipitation_ORIGIN'].apply(clean_precipitation)\n",
    "flight_V9_1['HourlyPrecipitation_DEST'] = flight_V9_1['HourlyPrecipitation_DEST'].apply(clean_precipitation)\n",
    "flight_V9_1['DailySnowDepth_DEST'] = flight_V9_1['DailySnowDepth_DEST'].apply(clean_precipitation)\n",
    "flight_V9_1['DailySnowfall_DEST'] = flight_V9_1['DailySnowfall_DEST'].apply(clean_precipitation)\n",
    "flight_V9_1['DailySnowDepth_ORIGIN'] = flight_V9_1['DailySnowDepth_ORIGIN'].apply(clean_precipitation)\n",
    "flight_V9_1['DailySnowfall_ORIGIN'] = flight_V9_1['DailySnowfall_ORIGIN'].apply(clean_precipitation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51caebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE EXTREME FOR ORIGIN AND DESTINATION\n",
    "# 定义极端天气的条件\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "extreme_weather_conditions_ORIGIN = (\n",
    "    (flight_V9_1['HourlyWindSpeed_ORIGIN'] > 13.9) |                    # 风速 > 13.9 m/s\n",
    "    (flight_V9_1['HourlyWindGustSpeed_ORIGIN'] > 11.1) |                # 风阵 > 11.1 m/s\n",
    "    ((flight_V9_1['HourlyRelativeHumidity_ORIGIN'] > 90) &              # 湿度 > 90% 且\n",
    "     (flight_V9_1['HourlyVisibility_ORIGIN'] < 2)) |                    # 能见度 < 2 km\n",
    "    (flight_V9_1['HourlyVisibility_ORIGIN'] < 2)| \n",
    "    (flight_V9_1['HourlyWetBulbTemperature_ORIGIN'] > 30) |             # 湿球温度 > 30°C\n",
    "    (flight_V9_1['HourlyWetBulbTemperature_ORIGIN'] < -15) |            # 湿球温度 < -15°C\n",
    "    (flight_V9_1['HourlyPrecipitation_ORIGIN'] > 10) |\n",
    "    (flight_V9_1['HourlySkyConditions_ORIGIN'].str.contains('BKN|OVC|VV', na=False))  # 云层状况包括BKN, OVC等\n",
    ")\n",
    "\n",
    "extreme_weather_conditions_DEST = (\n",
    "    (flight_V9_1['HourlyWindSpeed_DEST'] > 13.9) |                    # 风速 > 13.9 m/s\n",
    "    (flight_V9_1['HourlyWindGustSpeed_DEST'] > 11.1) |                # 风阵 > 11.1 m/s\n",
    "    ((flight_V9_1['HourlyRelativeHumidity_DEST'] > 90) &              # 湿度 > 90% 且\n",
    "     (flight_V9_1['HourlyVisibility_DEST'] < 2)) |                    # 能见度 < 2 km\n",
    "    (flight_V9_1['HourlyVisibility_DEST'] < 2)| \n",
    "    (flight_V9_1['HourlyWetBulbTemperature_DEST'] > 30) |             # 湿球温度 > 30°C\n",
    "    (flight_V9_1['HourlyWetBulbTemperature_DEST'] < -15) |            # 湿球温度 < -15°C\n",
    "    (flight_V9_1['HourlyPrecipitation_DEST'] > 10) |\n",
    "    (flight_V9_1['HourlySkyConditions_DEST'].str.contains('BKN|OVC|VV', na=False))  # 云层状况包括BKN, OVC等\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "flight_V9_1['EXTREME_ORIGIN'] = extreme_weather_conditions_ORIGIN.astype(int)\n",
    "flight_V9_1['EXTREME_DEST'] = extreme_weather_conditions_DEST.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e48538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b573da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各变量的缺失值数量\n",
    "flight_V9_1.isnull().sum().to_csv('V9_2_na.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05a3e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "符合极端天气条件的数据已保存至 flight_weather_data_V9_2.csv\n"
     ]
    }
   ],
   "source": [
    "# 输出至CSV文件\n",
    "output_path = \"flight_weather_data_V9_2.csv\"\n",
    "flight_V9_1.to_csv(output_path, index=True)\n",
    "\n",
    "print(f\"符合极端天气条件的数据已保存至 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80b19f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shixin/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (50) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "flight_V9_3 = pd.read_csv('flight_weather_data_V9_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f4adfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_V9_3[flight_V9_3['HourlyRelativeHumidity_DEST'].isna()].to_csv('V9_3_humidityMissing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74c30dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'index', 'YEAR', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK',\n",
       "       'MKT_CARRIER', 'MKT_CARRIER_FL_NUM', 'OP_UNIQUE_CARRIER',\n",
       "       'OP_CARRIER_AIRLINE_ID', 'TAIL_NUM', 'OP_CARRIER_FL_NUM', 'ORIGIN',\n",
       "       'ORIGIN_CITY_NAME', 'DEST', 'DEST_CITY_NAME', 'CRS_DEP_TIME',\n",
       "       'DEP_TIME', 'DEP_DELAY', 'TAXI_OUT', 'TAXI_IN', 'CRS_ARR_TIME',\n",
       "       'ARR_TIME', 'ARR_DELAY', 'CANCELLED', 'DIVERTED', 'ACTUAL_ELAPSED_TIME',\n",
       "       'DISTANCE', 'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY',\n",
       "       'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY', 'Timechange_CRS_DEP_Time',\n",
       "       'Timechange_FL_DATE', 'Timechange_ARR_Time', 'DATE_ORIGIN',\n",
       "       'NAME_ORIGIN', 'REPORT_TYPE_ORIGIN', 'HourlyPrecipitation_ORIGIN',\n",
       "       'HourlyRelativeHumidity_ORIGIN', 'HourlySkyConditions_ORIGIN',\n",
       "       'HourlyStationPressure_ORIGIN', 'HourlyVisibility_ORIGIN',\n",
       "       'HourlyWetBulbTemperature_ORIGIN', 'HourlyWindGustSpeed_ORIGIN',\n",
       "       'HourlyWindSpeed_ORIGIN', 'DailySnowDepth_ORIGIN',\n",
       "       'DailySnowfall_ORIGIN', 'DATE_DEST', 'NAME_DEST', 'REPORT_TYPE_DEST',\n",
       "       'HourlyPrecipitation_DEST', 'HourlyRelativeHumidity_DEST',\n",
       "       'HourlySkyConditions_DEST', 'HourlyStationPressure_DEST',\n",
       "       'HourlyVisibility_DEST', 'HourlyWetBulbTemperature_DEST',\n",
       "       'HourlyWindGustSpeed_DEST', 'HourlyWindSpeed_DEST',\n",
       "       'DailySnowDepth_DEST', 'DailySnowfall_DEST', 'ARR_Time_min',\n",
       "       'DEP_Time_min', 'Y', 'DELAYED', 'EXTREME_ORIGIN', 'EXTREME_DEST',\n",
       "       'Days_after_Thanksgiving', 'Days_after_Christmas',\n",
       "       'Days_after_New_Year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_V9_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e8b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 加载数据\n",
    "# data = df\n",
    "\n",
    "# # 将日期列转换为日期格式\n",
    "# data['Timechange_FL_DATE'] = pd.to_datetime(data['Timechange_FL_DATE'], errors='coerce')\n",
    "\n",
    "# # 提取年份，并处理1月的日期\n",
    "# data['Year'] = data['Timechange_FL_DATE'].dt.year\n",
    "# data.loc[data['Timechange_FL_DATE'].dt.month == 1, 'Year'] -= 1\n",
    "\n",
    "# # 定义节假日日期并转换成 DataFrame 格式\n",
    "# holiday_data = {\n",
    "#     'Year': list(range(2017, 2024)),\n",
    "#     'Thanksgiving': [datetime(year, 11, 23) if year == 2017 else datetime(year, 11, 22) if year == 2018 else \n",
    "#                      datetime(year, 11, 28) if year == 2019 else datetime(year, 11, 26) if year == 2020 else \n",
    "#                      datetime(year, 11, 25) if year == 2021 else datetime(year, 11, 24) if year == 2022 else \n",
    "#                      datetime(year, 11, 23) for year in range(2017, 2024)],\n",
    "#     'Christmas': [datetime(year, 12, 25) for year in range(2017, 2024)],\n",
    "#     'New Year': [datetime(year + 1, 1, 1) for year in range(2017, 2024)]\n",
    "# }\n",
    "# holiday_df = pd.DataFrame(holiday_data)\n",
    "\n",
    "# # 将节假日数据合并到原始数据上\n",
    "# data = data.merge(holiday_df, on='Year', how='left')\n",
    "\n",
    "# # 计算每个节假日的天数差异\n",
    "# data['Days_after_Thanksgiving'] = (data['Timechange_FL_DATE'] - data['Thanksgiving']).dt.days\n",
    "# data['Days_after_Christmas'] = (data['Timechange_FL_DATE'] - data['Christmas']).dt.days\n",
    "# data['Days_after_New_Year'] = (data['Timechange_FL_DATE'] - data['New Year']).dt.days\n",
    "\n",
    "# # 删除不需要的列\n",
    "# data.drop(columns=['Year', 'Thanksgiving', 'Christmas', 'New Year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify columns with mixed data types (float and str)\n",
    "# df = data\n",
    "# mixed_type_columns = []\n",
    "# for col in df.columns:\n",
    "#     # Check if the column has both float and str types\n",
    "#     unique_types = set(type(val) for val in df[col].dropna())\n",
    "#     if float in unique_types and str in unique_types:\n",
    "#         mixed_type_columns.append(col)\n",
    "#         print(f\"Column '{col}' has mixed types: {unique_types}\")\n",
    "\n",
    "# # Output columns with mixed types\n",
    "# if mixed_type_columns:\n",
    "#     print(\"\\nColumns with mixed data types (float and str):\")\n",
    "#     print(mixed_type_columns)\n",
    "# else:\n",
    "#     print(\"No columns with mixed data types (float and str) found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b583c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to clean the values\n",
    "# def clean_visibility(value):\n",
    "#     if isinstance(value, str):\n",
    "#         # If the value ends with 'V' or 's', remove the last character\n",
    "#         if value.endswith('V') or value.endswith('s'):\n",
    "#             value = value[:-1]\n",
    "#         # If the value is '*', consider it as missing (NaN)\n",
    "#         elif value == '*':\n",
    "#             return np.nan\n",
    "#     # Convert the cleaned value to float if possible\n",
    "#     try:\n",
    "#         return float(value)\n",
    "#     except ValueError:\n",
    "#         return np.nan  # Return NaN if conversion to float fails\n",
    "\n",
    "# # Apply the cleaning function to the column\n",
    "# data['HourlyVisibility_ORIGIN'] = data['HourlyVisibility_ORIGIN'].apply(clean_visibility)\n",
    "# data['HourlyVisibility_DEST'] = data['HourlyVisibility_DEST'].apply(clean_visibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a7ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to clean the values\n",
    "# def clean_windspeed(value):\n",
    "#     if isinstance(value, str):\n",
    "#         # If the value ends with 'V' or 's', remove the last character\n",
    "#         if value.endswith('s'):\n",
    "#             value = value[:-1]\n",
    "#     # Convert the cleaned value to float if possible\n",
    "#     try:\n",
    "#         return float(value)\n",
    "#     except ValueError:\n",
    "#         return np.nan  # Return NaN if conversion to float fails\n",
    "\n",
    "# # Apply the cleaning function to the column\n",
    "# data['HourlyWindSpeed_ORIGIN'] = data['HourlyWindSpeed_ORIGIN'].apply(clean_windspeed)\n",
    "# data['HourlyWindSpeed_DEST'] = data['HourlyWindSpeed_DEST'].apply(clean_windspeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to clean the values\n",
    "# def clean_precipitation(value):\n",
    "#     if isinstance(value, str):\n",
    "#         # If the value ends with 'V' or 's', remove the last character\n",
    "#         if value=='T':\n",
    "#             value = 0.01\n",
    "#     # Convert the cleaned value to float if possible\n",
    "#     try:\n",
    "#         return float(value)\n",
    "#     except ValueError:\n",
    "#         return np.nan  # Return NaN if conversion to float fails\n",
    "\n",
    "# # Apply the cleaning function to the column\n",
    "# data['HourlyPrecipitation_DEST'] = data['HourlyPrecipitation_DEST'].apply(clean_precipitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义极端天气的条件\n",
    "# extreme_weather_conditions = (\n",
    "#     (data['HourlyWindSpeed_DEST'] > 13.9) |                    # 风速 > 13.9 m/s\n",
    "#     (data['HourlyWindGustSpeed_DEST'] > 11.1) |                # 风阵 > 11.1 m/s\n",
    "#     ((data['HourlyRelativeHumidity_DEST'] > 90) &              # 湿度 > 90% 且\n",
    "#      (data['HourlyVisibility_DEST'] < 2)) |                    # 能见度 < 2 km\n",
    "#     (data['HourlyVisibility_DEST'] < 2)| \n",
    "#     (data['HourlyWetBulbTemperature_DEST'] > 30) |             # 湿球温度 > 30°C\n",
    "#     (data['HourlyWetBulbTemperature_DEST'] < -15) |            # 湿球温度 < -15°C\n",
    "#     (data['HourlyPrecipitation_DEST'] > 10) |\n",
    "#     (data['HourlySkyConditions_DEST'].str.contains('BKN|OVC|VV', na=False))  # 云层状况包括BKN, OVC等\n",
    "# )\n",
    "\n",
    "# # 新增一列“EXTREME_WEATHER\"，满足条件为1，否则为0\n",
    "# data['EXTREME_WEATHER'] = extreme_weather_conditions.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b60539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7a28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e45af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2caa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
